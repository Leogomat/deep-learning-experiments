{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining - Assignment 3 (35 points total)\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. \n",
    "Assume the code is run under python 3.8.\n",
    "You can use libraries like pytorch, torchvision, torchtext, numpy, nltk, sklearn, and all python standard libraries.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 3-4 people until 18.01.2022 23:59CET. The deadline is strict!**\n",
    "\n",
    "Deadline for early submission (prelim feedback): 09.01.2022 23:59.\n",
    "\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi-automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for only the mean, prints an output instead of returning it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Leonardo',\n",
    "        'last_name': 'Gomes da Matta e Silva',\n",
    "        'student_id': 384657\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Florisa',\n",
    "        'last_name': 'Zanier',\n",
    "        'student_id': 317700\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Felix',\n",
    "        'last_name': 'Paulig',\n",
    "        'student_id': 394924\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Lisa',\n",
    "        'last_name': 'PÃ¼hl',\n",
    "        'student_id': 394649\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task\n",
    "We will have a \"kaggle\"-like competition within the class. You will be given a task and you will have to find a solution that maximizes the accuracy on some heldout test set.\n",
    "This home assignment will allow you more freedoms then those that you have previously encountered.\n",
    "\n",
    "The goal is to perform text classification. You are given a set of small text snippets, each with a label (0 to 5, both included) that you shall use for training.\n",
    "You model will be evaluated on a heldout dataset. You provide us (among other things) with this notebook which is used to load your pretrained model (which is then scored according to accuracy on the heldout data) and to run some other simple tests.\n",
    "\n",
    "You will have to repeat these steps for both one RNN and one CNN architecture. \n",
    "- Write a dataset class in order to load the data.\n",
    "- Write a model class that is called `RNN`/`CNN` and has an init function which works when called without arguments.\n",
    "- Write a function `get_<rnn/cnn>_dataset` that takes a path to a dataset file. This file will be in JSONL format and each line will contain a string stored at key `\"text\"` and an integer label at key `\"label\"`\n",
    "- Write a function `train_<rnn/cnn>` which performs training of your RNN/CNN architecture.\n",
    "\n",
    "Please experiment with different RNN/CNN architectures and find one which yields good accuracy.\n",
    "Also, provide a report (PDF format) which contains a visualization of your architecture and corresponding links/citations if you took inspiration from other code/literature for this assignment.\n",
    "\n",
    "\n",
    "This home assignment will be worth 35 points distributed as follows:\n",
    "\n",
    "- 4 + 4 points for report (4+4 means 4 for RNN, 4 for CNN)\n",
    " - visualization of final architecture is included (see e.g. https://github.com/szagoruyko/pytorchviz, not optimal but a starting point)\n",
    "- 8 + 8 points for a working architecture\n",
    " - model can be loaded\n",
    " - model can be initialized (by calling `RNN()`/`CNN()`)\n",
    " - model can be trained\n",
    " - you provide a dataset class\n",
    "- 11 (up to) points for accuracy of your model\n",
    "\n",
    "|$<55$|$\\geq 55$|$\\geq 60$|$\\geq65$|$\\geq70$|$\\geq75$|$\\geq80$|$\\geq85$|$\\geq90$|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|$0$|$4$  |$5$ |$6$  |$7$ |$8$  |$9$ |$10$  |$11$ |\n",
    "\n",
    "\n",
    "It is very important that your model can be loaded!\n",
    "\n",
    "Please submit the following files:\n",
    "- Your jupyter notebook (make sure you have provided **credentials**)\n",
    "- A report (PDF) containing visualizations for both your models\n",
    "- `rnn.pt` (trained model parameters for `RNN`, see code below)\n",
    "- `cnn.pt` (trained model parameters for `CNN`, see code below)\n",
    "- `words2index_rnn.csv`, `words2index_cnn.csv` files that, if necessary, are used to map words to indices. In principle you can put any kind of data here (you are the only one responsible for working with this data), but please stick to these filenames including their type indications even though you saved e.g. a pickle'd object.\n",
    "\n",
    "If your files `rnn.pt` and `cnn.pt` exceed 250MB then please provide us with a link to the files by putting the link into `rnn.txt` and/or `cnn.txt`. We will not go to length to get the files, though. We will simply run: `wget -i rnn.txt -O rnn.pt`. Therefore, make sure that one can get the files through this method before submission if this case applies to your solutions!\n",
    "\n",
    "You are allowed to use a GPU for training, but make sure your model is loaded into CPU/trained on CPU by default. You are also allowed to take inspiration from literature, you are not allowed to straight up copy and paste code though. If you take inspiration, specify references here in the code and in your report. You are allowed to use tricks of the trade for training. If you want to use pretrained wordvectors you are only allowed to use those provided by torchtext (e.g. https://pytorch.org/text/stable/vocab.html#glove)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "from torchtext.vocab import GloVe\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(word2index_path='words2index_cnn.csv', dim=300):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings as a tensor. If a words2index file is given, it determines the index of the embedding of each word in the resulting\n",
    "    tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load GloVe embeddings\n",
    "    glove = GloVe(name='6B', dim=dim)\n",
    "\n",
    "    # Load word2index dictionary\n",
    "    word2index = {}\n",
    "    with open(word2index_path, 'r',  encoding=\"utf-8\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        word2index = {row[0]:int(row[1]) for row in csv_reader}\n",
    "\n",
    "    # Initialize embeddings (words not in glove vocabulary are randomly initialized, padding is zero vector, unknown token is mean embedding)\n",
    "    embeddings = np.random.uniform(-0.25,0.25,(len(word2index),dim))\n",
    "    for word in word2index.keys():\n",
    "        if word in glove.stoi:\n",
    "            embeddings[word2index[word]] = glove[word]\n",
    "    embeddings[word2index['<pad>']] = np.zeros(dim)\n",
    "    embeddings[word2index['<unk>']] = np.mean(embeddings, axis=0)\n",
    "\n",
    "    return torch.FloatTensor(embeddings)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture inspired by the original paper on convolutional networks for sentence classification (Kim, 2014).\n",
    "    Architecture and embedding implementation details inspired by Tran in https://chriskhanhtran.github.io/posts/cnn-sentence-classification/.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings_path='words2index_cnn.csv'):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = 300\n",
    "        self.embeddings = load_embeddings(embeddings_path,self.embedding_dim) # Pretrained word embeddings\n",
    "        self.vocab_size, self.embeddings_dim = self.embeddings.shape # Dimension of embeddings and size of vocabulary\n",
    "        self.filter_sizes = [2,3,4] # Size of filter kernels\n",
    "        self.n_filters = [100,100,100] # Number of filters for each kernel size\n",
    "        self.n_classes = 6 # Number of classes for output\n",
    "        self.dropout_p = 0.5 # Dropout probability\n",
    "        self.freeze_embedding = True # Fix embeddings or not\n",
    "\n",
    "        # Initialize embedding layer\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(self.embeddings, freeze=self.freeze_embedding)\n",
    "\n",
    "        # Initialize convolution layer\n",
    "        self.convolution_layer = nn.ModuleList([nn.Conv1d(in_channels=self.embeddings_dim,out_channels=self.n_filters[i],kernel_size=self.filter_sizes[i]) for i in range(len(self.filter_sizes))])\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fully_connected_layer = nn.Linear(np.sum(self.n_filters),self.n_classes)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Get embeddings of input indexes and reshape tensor to match convolution requirement\n",
    "        x_embedding = self.embedding_layer(input).permute(0,2,1)\n",
    "        \n",
    "        # Apply convolution layer\n",
    "        x_convolutions = [conv(x_embedding) for conv in self.convolution_layer]\n",
    "\n",
    "        # Apply ReLU\n",
    "        x_relu = [F.relu(conv) for conv in x_convolutions]\n",
    "\n",
    "        # Max-over-time-pooling\n",
    "        x_pooling = [F.max_pool1d(conv, kernel_size=conv.shape[2]) for conv in x_relu]\n",
    "\n",
    "        # Concatenate pooling results\n",
    "        x_concatenated = torch.cat([pool.squeeze(dim=2) for pool in x_pooling], dim=1)\n",
    "\n",
    "        # Feed concatenated results into fully connected layer\n",
    "        x_fully_connected = self.fully_connected_layer(self.dropout_layer(x_concatenated))\n",
    "\n",
    "        return x_fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    One-directed LSTM for sentence classification.\n",
    "    Implementation details inspired by Cheng in https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings_path='words2index_rnn.csv'):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = 300\n",
    "        self.embeddings = load_embeddings(embeddings_path,self.embedding_dim) # Pretrained word embeddings\n",
    "        self.vocab_size, self.embeddings_dim = self.embeddings.shape # Dimension of embeddings and size of vocabulary\n",
    "        self.n_classes = 6 # Number of classes for output\n",
    "        self.dropout_p = 0.2 # Dropout probability\n",
    "        self.hidden_dim = 128 # Dimension of hidden states\n",
    "\n",
    "        # Initialize embedding layer\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(self.embeddings)\n",
    "\n",
    "        # Initialize bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim,hidden_size=self.hidden_dim,bidirectional=False,batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fully_connected_layer = nn.Linear(self.hidden_dim,self.n_classes)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Separate input\n",
    "        x = input[0]\n",
    "        sent_lens = input[1]\n",
    "\n",
    "        # Get embeddings of input indexes and reshape tensor to match convolution requirement\n",
    "        x_embedding = self.embedding_layer(x)\n",
    "\n",
    "        # Pack input sentences\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(x_embedding, sent_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Apply LSTM\n",
    "        x_lstm, _ = self.lstm(x_packed)\n",
    "\n",
    "        # Repad output\n",
    "        x_padded, sent_lens = nn.utils.rnn.pad_packed_sequence(x_lstm, batch_first=True)\n",
    "\n",
    "        # Extract hidden states from forward and backwards pass\n",
    "        x_forward = x_padded[torch.arange(len(x_padded)),sent_lens - 1,:]\n",
    "        #x_backwards = x_padded[:,0,self.hidden_dim:]\n",
    "        #x_concatenated = torch.cat((x_forward,x_backwards),1)\n",
    "        \n",
    "\n",
    "        # Feed concatenated results into fully connected layer (with dropout)\n",
    "        x_fully_connected = self.fully_connected_layer(self.dropout_layer(x_forward))\n",
    "\n",
    "        return x_fully_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "For reading the `JSONL` file consider using the `json` library from python (in particular `.loads`) and a loop over the lines of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "class cnn_dataset_class(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for training and evaluating our CNN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize data\n",
    "    def __init__(self, data_points, class_labels):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "        # Define data points and labels\n",
    "        self.X = data_points\n",
    "        self.y = class_labels\n",
    "\n",
    "        # Define number of samples\n",
    "        self.n_samples = len(data_points)\n",
    "\n",
    "    # Indexing\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.X[index]), torch.tensor(self.y[index])\n",
    "\n",
    "    # Returns length of dataset\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "def get_cnn_dataset(path : str, optional_file : str = 'word2index_cnn.csv'):\n",
    "    \"\"\"\n",
    "    Data preprocessing function. Sentences are tokenized and transformed into sequences of indices.\n",
    "    If a word2index file is provided, it is used to determine the indices of the words.\n",
    "    Returns a Dataset object that can be used for training and evaluating our CNN.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = []\n",
    "    class_labels = []\n",
    "    with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "        json_list = list(file)\n",
    "        for line in json_list:\n",
    "            json_obj = json.loads(line)\n",
    "            sentences.append(json_obj['text'])\n",
    "            class_labels.append(json_obj['label'])\n",
    "\n",
    "    # Tokenize sentences\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(r'<.*?>','',sentence)\n",
    "        sent_tokens = nltk.word_tokenize(sentence)#[:300]\n",
    "        if len(sent_tokens) != 0:\n",
    "            tokenized_sentences.append(sent_tokens)\n",
    "\n",
    "    # Create vocabulary if no word2index file is available\n",
    "    word2index = {}\n",
    "    if optional_file != None and os.path.exists(optional_file):\n",
    "        with open(optional_file, 'r',  encoding=\"utf-8\") as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            word2index = {row[0]:int(row[1]) for row in csv_reader}\n",
    "    else:\n",
    "        word2index['<pad>'] = 0\n",
    "        word2index['<unk>'] = 1\n",
    "        index_count = 2\n",
    "        for sentence in tokenized_sentences:\n",
    "            for token in sentence:\n",
    "                if token not in word2index:\n",
    "                    word2index[token] = index_count\n",
    "                    index_count += 1\n",
    "        \n",
    "        # Save vocabulary as csv file\n",
    "        with open('words2index_cnn.csv', 'w',  encoding=\"utf-8\", newline='') as file:\n",
    "            csv_writer = csv.writer(file)\n",
    "            for key in word2index.keys():\n",
    "                csv_writer.writerow([key, word2index[key]])\n",
    "\n",
    "    # Encode sentences using vocabulary\n",
    "    data_points = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        data_points.append([word2index.get(token, word2index['<unk>']) for token in sentence])\n",
    "\n",
    "    return cnn_dataset_class(data_points, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def cnn_collate(batch):\n",
    "    \"\"\"\n",
    "    Collate function that pads all sequences in the batch.\n",
    "    \"\"\"\n",
    "    data, labels = [], []\n",
    "    for datapoint in batch:\n",
    "        data.append(datapoint[0])\n",
    "        labels.append(datapoint[1])\n",
    "\n",
    "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "\n",
    "    return torch.LongTensor(data), torch.tensor(labels)\n",
    "\n",
    "cnn_collate_fn = cnn_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn_dataset_class(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for training and evaluating our RNN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize data\n",
    "    def __init__(self, data_points, class_labels):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "        # Define data points and labels\n",
    "        self.X = data_points\n",
    "        self.y = class_labels\n",
    "\n",
    "        # Define number of samples\n",
    "        self.n_samples = len(data_points)\n",
    "\n",
    "    # Indexing\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.X[index]), torch.tensor(self.y[index])\n",
    "\n",
    "    # Returns length of dataset\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "def get_rnn_dataset(path : str, optional_file : str = 'word2index_rnn.csv'):\n",
    "    \"\"\"\n",
    "    Data preprocessing function. Sentences are tokenized and transformed into sequences of indices.\n",
    "    If a word2index file is provided, it is used to determine the indices of the words.\n",
    "    Returns a Dataset object that can be used for training and evaluating our RNN.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = []\n",
    "    class_labels = []\n",
    "    with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "        json_list = list(file)\n",
    "        for line in json_list:\n",
    "            json_obj = json.loads(line)\n",
    "            sentences.append(json_obj['text'])\n",
    "            class_labels.append(json_obj['label'])\n",
    "\n",
    "    # Tokenize sentences\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(r'<.*?>','',sentence)\n",
    "        sent_tokens = nltk.word_tokenize(sentence)\n",
    "        if len(sent_tokens) != 0:\n",
    "            tokenized_sentences.append(sent_tokens)\n",
    "\n",
    "    # Create vocabulary if no word2index file is available\n",
    "    word2index = {}\n",
    "    if optional_file != None and os.path.exists(optional_file):\n",
    "        with open(optional_file, 'r',  encoding=\"utf-8\") as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            word2index = {row[0]:int(row[1]) for row in csv_reader}\n",
    "    else:\n",
    "        word2index['<pad>'] = 0\n",
    "        word2index['<unk>'] = 1\n",
    "        index_count = 2\n",
    "        for sentence in tokenized_sentences:\n",
    "            for token in sentence:\n",
    "                if token not in word2index:\n",
    "                    word2index[token] = index_count\n",
    "                    index_count += 1\n",
    "        \n",
    "        # Save vocabulary as csv file\n",
    "        with open('words2index_rnn.csv', 'w',  encoding=\"utf-8\", newline='') as file:\n",
    "            csv_writer = csv.writer(file)\n",
    "            for key in word2index.keys():\n",
    "                csv_writer.writerow([key, word2index[key]])\n",
    "    \n",
    "    # Encode sentences using vocabulary\n",
    "    data_points = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        data_points.append([word2index.get(token, word2index['<unk>']) for token in sentence])\n",
    "        \n",
    "    return rnn_dataset_class(data_points, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def rnn_collate(batch):\n",
    "    \"\"\"\n",
    "    Collate function that pads all sequences in the batch and also returns the sentence lengths.\n",
    "    \"\"\"\n",
    "    data, lengths, labels = [], [], []\n",
    "    for datapoint in batch:\n",
    "        data.append(datapoint[0])\n",
    "        lengths.append(len(datapoint[0]))\n",
    "        labels.append(datapoint[1])\n",
    "\n",
    "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "\n",
    "    return [torch.LongTensor(data), lengths], torch.tensor(labels)\n",
    "\n",
    "rnn_collate_fn = rnn_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device for GPU usage\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "\n",
    "def train_cnn(cnn_instance : CNN, dataset, max_train_time : float, use_gpu=False):\n",
    "    \"\"\"Performs training of the CNN for at most max_train_time\n",
    "    You do not have to go to strict about max_train_time but try to stick to that limit\n",
    "    e.g. if max_train_time=1s and after 10min its still going we would consider this to be a fail\n",
    "    \"\"\"\n",
    "    end_time = time.time() + max_train_time # keep\n",
    "    \n",
    "    # Set training parameters\n",
    "    batch_size = 32\n",
    "    epochs = 25\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 0.0001\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(cnn_instance.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Define loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set model to training mode\n",
    "    cnn_instance.train()\n",
    "\n",
    "    # If GPU is to be used, send model to device\n",
    "    if use_gpu:\n",
    "        cnn_instance.to(device)\n",
    "\n",
    "    for i in range(epochs): # modify to create real training loop\n",
    "        dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True,collate_fn=cnn_collate_fn)\n",
    "        total_loss = 0\n",
    "        for data, labels in dataloader:\n",
    "            # Break out of training loop if the maximum training time has been reached\n",
    "            if time.time() > end_time:\n",
    "                break\n",
    "            \n",
    "            # If GPU is to be used, send tensors to device\n",
    "            if use_gpu:\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            # Clear out previous gradients\n",
    "            cnn_instance.zero_grad()\n",
    "\n",
    "            # Perform forward pass\n",
    "            output = cnn_instance(data)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(output, labels)\n",
    "            total_loss += loss * batch_size\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "        # Break out of training loop if the maximum training time has been reached\n",
    "        if time.time() > end_time:\n",
    "            break\n",
    "\n",
    "        print(f'Total loss for epoch {i+1}: {total_loss.item()}')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may add additional parameters with default values\n",
    "\n",
    "def train_rnn(rnn_instance : RNN, dataset, max_train_time : float, use_gpu=False):\n",
    "    \"\"\"Performs training of the CNN for at most max_train_time\n",
    "    You do not have to go to strict about max_train_time but try to stick to that limit\n",
    "    e.g. if max_train_time=1s and after 10min its still going we would consider this to be a fail\n",
    "    \"\"\"\n",
    "    end_time = time.time() + max_train_time # keep\n",
    "    \n",
    "    # Set training parameters\n",
    "    batch_size = 32\n",
    "    epochs = 30\n",
    "    learning_rate = 0.003\n",
    "    weight_decay = 0.0001\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(rnn_instance.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "\n",
    "    # Define loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set model to training mode\n",
    "    rnn_instance.train()\n",
    "\n",
    "    # If GPU is to be used, send model to device\n",
    "    if use_gpu:\n",
    "        rnn_instance.to(device)\n",
    "\n",
    "    for i in range(epochs): # modify to create real training loop\n",
    "        dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True,collate_fn=rnn_collate)\n",
    "        total_loss = 0\n",
    "        for data, labels in dataloader:\n",
    "            # Break out of training loop if the maximum training time has been reached\n",
    "            if time.time() > end_time:\n",
    "                break\n",
    "\n",
    "            # If GPU is to be used, send tensors to device\n",
    "            if use_gpu:\n",
    "                data[0] = data[0].to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            # Clear out previous gradients\n",
    "            rnn_instance.zero_grad()\n",
    "\n",
    "            # Perform forward pass\n",
    "            output = rnn_instance(data)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(output, labels)\n",
    "            total_loss += loss * batch_size\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "        # Break out of training loop if the maximum training time has been reached\n",
    "        if time.time() > end_time:\n",
    "            break\n",
    "\n",
    "        print(f'Total loss for epoch {i+1}: {total_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#random_seed = 2\n",
    "\n",
    "#torch.cuda.manual_seed_all(random_seed)\n",
    "#torch.manual_seed(random_seed)\n",
    "#random.seed(random_seed)\n",
    "#np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "#cnn_dataset = get_cnn_dataset('trainfile.jsonl','words2index_cnn.csv')\n",
    "#train_size = int(cnn_dataset.__len__() * 0.9)\n",
    "#test_size = cnn_dataset.__len__() - train_size\n",
    "#cnn_train, cnn_test = torch.utils.data.random_split(cnn_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "#cnn_dataset_reduced = get_cnn_dataset('trainfile_reduced.jsonl','words2index_cnn.csv')\n",
    "#train_size_reduced = int(cnn_dataset_reduced.__len__() * 0.9)\n",
    "#test_size_reduced = cnn_dataset_reduced.__len__() - train_size_reduced\n",
    "#cnn_train_reduced, cnn_test_reduced = torch.utils.data.random_split(cnn_dataset_reduced, [train_size_reduced, test_size_reduced], generator=torch.Generator().manual_seed(random_seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_inst = CNN() # make sure this works\n",
    "#cnn_dataset = get_cnn_dataset(\"trainfile.jsonl\") # make sure this works (potentially also different foldername)\n",
    "#cnn_dataset = get_cnn_dataset(\"trainfile.jsonl\", 'word2index_cnn.csv') # make sure this works (potentially also different foldername)\n",
    "train_cnn(cnn_inst, cnn_dataset, 30, use_gpu=False) # make sure this works\n",
    "torch.save(cnn_inst.state_dict(), \"cnn.pt\")# save model after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn_dataset = get_rnn_dataset('trainfile.jsonl','words2index_rnn.csv')\n",
    "#train_size = int(rnn_dataset.__len__() * 0.9)\n",
    "#test_size = rnn_dataset.__len__() - train_size\n",
    "#rnn_train, rnn_test = torch.utils.data.random_split(rnn_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "#rnn_dataset_reduced = get_rnn_dataset('trainfile_reduced.jsonl','words2index_rnn.csv')\n",
    "#train_size_reduced = int(rnn_dataset_reduced.__len__() * 0.9)\n",
    "#test_size_reduced = rnn_dataset_reduced.__len__() - train_size_reduced\n",
    "#rnn_train_reduced, rnn_test_reduced = torch.utils.data.random_split(rnn_dataset_reduced, [train_size_reduced, test_size_reduced], generator=torch.Generator().manual_seed(random_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_inst = RNN() # make sure this works\n",
    "#rnn_dataset = get_rnn_dataset(\"trainfile.jsonl\") # make sure this works (potentially also different foldername)\n",
    "#rnn_dataset = get_rnn_dataset(\"trainfile.jsonl\", 'word2index_rnn.csv') # make sure this works (potentially also different foldername)\n",
    "train_rnn(rnn_inst, rnn_dataset, 30, use_gpu=False) # make sure this works\n",
    "torch.save(rnn_inst.state_dict(), \"rnn.pt\")# save model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here for demonstration, may remove\n",
    "torch.save(cnn_inst.state_dict(), \"cnn.pt\")# save model after training \n",
    "torch.save(rnn_inst.state_dict(), \"rnn.pt\")# save model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure your trained NN can be evaluated using this function here\n",
    "# DO NOT MODIFY!\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "def evaluate(clf, test_data, batch_size=100, collate_fn=None):\n",
    "    \n",
    "    true_labels = []\n",
    "    inf_labels = []\n",
    "    clf.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn):\n",
    "            out = clf(data)\n",
    "            cls = torch.argmax(F.softmax(out, dim=1), dim=1)\n",
    "            inf_labels.extend(cls.detach().numpy().tolist())\n",
    "            true_labels.extend(labels.numpy().tolist())\n",
    "    clf.train()\n",
    "    return accuracy_score(true_labels, inf_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here for demonstration, may remove\n",
    "# This demonstrates how we plan on evaluating the model\n",
    "cnn_loaded = CNN()                               # make sure you can init model without parameters\n",
    "cnn_loaded.load_state_dict(torch.load(\"./cnn.pt\"))  # and you can load it afterwards\n",
    "cnn_loaded.eval() # make sure this works\n",
    "#cnn_dataset = get_cnn_dataset(\"trainfile.jsonl\", 'words2index_cnn.csv') # make sure this works (potentially also different filenames)\n",
    "print(evaluate(cnn_loaded, cnn_dataset, collate_fn=cnn_collate_fn)) # should work\n",
    "#print(evaluate(cnn_loaded, cnn_train, collate_fn=cnn_collate_fn)) # should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here for demonstration, may remove\n",
    "# This demonstrates how we plan on evaluating the model\n",
    "rnn_loaded = RNN() \n",
    "rnn_loaded.load_state_dict(torch.load(\"./rnn.pt\")) # make sure this works (potentially also different filename)\n",
    "rnn_loaded.eval() # make sure this works\n",
    "#rnn_dataset = get_rnn_dataset(\"train_file.jsonl\", '/some/path/optional_rnn.csv') # make sure this works (potentially also different foldername)\n",
    "print(evaluate(rnn_loaded, rnn_dataset, collate_fn=rnn_collate_fn)) # should workd\n",
    "#print(evaluate(rnn_loaded, rnn_train, collate_fn=rnn_collate_fn)) # should work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
